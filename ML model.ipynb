{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf83e1a4-65fc-4cc7-b9db-39995665a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.03\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         6\n",
      "           3       0.00      0.00      0.00        11\n",
      "           4       0.00      0.00      0.00        45\n",
      "           5       0.00      0.00      0.00        74\n",
      "           6       0.00      0.00      0.00       164\n",
      "           7       0.00      0.00      0.00       276\n",
      "           8       0.00      0.00      0.00       407\n",
      "           9       0.06      0.05      0.05       544\n",
      "          10       0.00      0.00      0.00       699\n",
      "          11       0.07      0.03      0.04       900\n",
      "          12       0.09      0.03      0.05      1117\n",
      "          13       0.07      0.04      0.05      1396\n",
      "          14       0.08      0.08      0.08      1559\n",
      "          15       0.06      0.04      0.05      1715\n",
      "          16       0.07      0.10      0.08      2014\n",
      "          17       0.06      0.08      0.07      2084\n",
      "          18       0.05      0.05      0.05      2342\n",
      "          19       0.06      0.09      0.07      2399\n",
      "          20       0.05      0.05      0.05      2484\n",
      "          21       0.05      0.03      0.04      2652\n",
      "          22       0.05      0.05      0.05      2709\n",
      "          23       0.05      0.07      0.06      2786\n",
      "          24       0.04      0.05      0.04      2676\n",
      "          25       0.04      0.05      0.04      2700\n",
      "          26       0.05      0.04      0.04      2816\n",
      "          27       0.04      0.04      0.04      2833\n",
      "          28       0.05      0.04      0.04      2883\n",
      "          29       0.04      0.03      0.04      2797\n",
      "          30       0.04      0.05      0.04      2953\n",
      "          31       0.04      0.04      0.04      2818\n",
      "          32       0.04      0.04      0.04      2719\n",
      "          33       0.04      0.06      0.05      2752\n",
      "          34       0.04      0.03      0.03      2787\n",
      "          35       0.03      0.02      0.02      2702\n",
      "          36       0.04      0.05      0.04      2740\n",
      "          37       0.04      0.02      0.03      2710\n",
      "          38       0.03      0.05      0.04      2566\n",
      "          39       0.03      0.02      0.03      2650\n",
      "          40       0.03      0.04      0.03      2550\n",
      "          41       0.03      0.04      0.03      2531\n",
      "          42       0.03      0.04      0.03      2506\n",
      "          43       0.03      0.02      0.02      2595\n",
      "          44       0.02      0.01      0.01      2562\n",
      "          45       0.02      0.03      0.02      2461\n",
      "          46       0.02      0.02      0.02      2372\n",
      "          47       0.03      0.03      0.03      2316\n",
      "          48       0.02      0.04      0.03      2338\n",
      "          49       0.02      0.02      0.02      2332\n",
      "          50       0.03      0.03      0.03      2349\n",
      "          51       0.02      0.03      0.03      2288\n",
      "          52       0.02      0.02      0.02      2202\n",
      "          53       0.02      0.02      0.02      2154\n",
      "          54       0.02      0.01      0.02      2268\n",
      "          55       0.02      0.03      0.03      2134\n",
      "          56       0.02      0.02      0.02      2081\n",
      "          57       0.03      0.03      0.03      2135\n",
      "          58       0.02      0.03      0.02      2045\n",
      "          59       0.02      0.03      0.02      1981\n",
      "          60       0.02      0.02      0.02      1987\n",
      "          61       0.03      0.04      0.03      1962\n",
      "          62       0.02      0.02      0.02      1859\n",
      "          63       0.02      0.02      0.02      1829\n",
      "          64       0.02      0.05      0.03      1847\n",
      "          65       0.02      0.03      0.02      1703\n",
      "          66       0.02      0.01      0.02      1734\n",
      "          67       0.02      0.03      0.02      1789\n",
      "          68       0.02      0.03      0.02      1618\n",
      "          69       0.02      0.03      0.02      1668\n",
      "          70       0.02      0.04      0.02      1583\n",
      "          71       0.02      0.01      0.02      1544\n",
      "          72       0.02      0.03      0.02      1472\n",
      "          73       0.02      0.02      0.02      1450\n",
      "          74       0.02      0.02      0.02      1479\n",
      "          75       0.02      0.03      0.02      1454\n",
      "          76       0.02      0.03      0.02      1299\n",
      "          77       0.01      0.00      0.01      1313\n",
      "          78       0.02      0.02      0.02      1328\n",
      "          79       0.01      0.01      0.01      1285\n",
      "          80       0.01      0.01      0.01      1238\n",
      "          81       0.02      0.01      0.01      1115\n",
      "          82       0.02      0.03      0.03      1148\n",
      "          83       0.02      0.02      0.02      1016\n",
      "          84       0.01      0.01      0.01      1134\n",
      "          85       0.02      0.01      0.01      1042\n",
      "          86       0.02      0.02      0.02      1058\n",
      "          87       0.02      0.04      0.02       983\n",
      "          88       0.02      0.02      0.02       991\n",
      "          89       0.01      0.01      0.01       934\n",
      "          90       0.01      0.01      0.01       923\n",
      "          91       0.01      0.02      0.01       853\n",
      "          92       0.02      0.01      0.01       851\n",
      "          93       0.02      0.03      0.02       780\n",
      "          94       0.01      0.01      0.01       750\n",
      "          95       0.01      0.01      0.01       742\n",
      "          96       0.01      0.02      0.01       719\n",
      "          97       0.01      0.00      0.01       699\n",
      "          98       0.01      0.02      0.02       663\n",
      "          99       0.00      0.00      0.00       597\n",
      "         100       0.00      0.00      0.00       644\n",
      "         101       0.00      0.00      0.00       609\n",
      "         102       0.00      0.00      0.00       579\n",
      "         103       0.02      0.05      0.03       571\n",
      "         104       0.01      0.00      0.00       552\n",
      "         105       0.01      0.02      0.01       513\n",
      "         106       0.00      0.00      0.00       491\n",
      "         107       0.02      0.02      0.02       494\n",
      "         108       0.00      0.00      0.00       447\n",
      "         109       0.00      0.00      0.00       469\n",
      "         110       0.01      0.02      0.02       442\n",
      "         111       0.01      0.01      0.01       397\n",
      "         112       0.00      0.00      0.00       372\n",
      "         113       0.02      0.02      0.02       373\n",
      "         114       0.00      0.00      0.00       324\n",
      "         115       0.00      0.00      0.00       340\n",
      "         116       0.01      0.03      0.02       310\n",
      "         117       0.00      0.00      0.00       308\n",
      "         118       0.00      0.00      0.00       290\n",
      "         119       0.00      0.00      0.00       275\n",
      "         120       0.00      0.00      0.00       245\n",
      "         121       0.00      0.00      0.00       251\n",
      "         122       0.00      0.00      0.00       233\n",
      "         123       0.00      0.00      0.00       210\n",
      "         124       0.00      0.00      0.00       246\n",
      "         125       0.00      0.00      0.00       215\n",
      "         126       0.00      0.00      0.00       169\n",
      "         127       0.00      0.00      0.00       180\n",
      "         128       0.00      0.00      0.00       183\n",
      "         129       0.00      0.00      0.00       203\n",
      "         130       0.00      0.00      0.00       171\n",
      "         131       0.00      0.00      0.00       150\n",
      "         132       0.00      0.00      0.00       147\n",
      "         133       0.00      0.00      0.00       125\n",
      "         134       0.00      0.00      0.00       149\n",
      "         135       0.00      0.00      0.00       118\n",
      "         136       0.00      0.00      0.00       109\n",
      "         137       0.00      0.00      0.00       115\n",
      "         138       0.00      0.00      0.00        94\n",
      "         139       0.00      0.00      0.00        75\n",
      "         140       0.00      0.00      0.00        90\n",
      "         141       0.00      0.00      0.00        92\n",
      "         142       0.00      0.00      0.00        74\n",
      "         143       0.00      0.00      0.00        71\n",
      "         144       0.00      0.00      0.00        77\n",
      "         145       0.00      0.00      0.00        76\n",
      "         146       0.00      0.00      0.00        60\n",
      "         147       0.00      0.00      0.00        50\n",
      "         148       0.00      0.00      0.00        68\n",
      "         149       0.00      0.00      0.00        42\n",
      "         150       0.00      0.00      0.00        51\n",
      "         151       0.00      0.00      0.00        35\n",
      "         152       0.00      0.00      0.00        42\n",
      "         153       0.00      0.00      0.00        43\n",
      "         154       0.00      0.00      0.00        50\n",
      "         155       0.00      0.00      0.00        25\n",
      "         156       0.00      0.00      0.00        28\n",
      "         157       0.00      0.00      0.00        21\n",
      "         158       0.00      0.00      0.00        33\n",
      "         159       0.00      0.00      0.00        15\n",
      "         160       0.00      0.00      0.00        27\n",
      "         161       0.00      0.00      0.00        11\n",
      "         162       0.00      0.00      0.00        16\n",
      "         163       0.00      0.00      0.00        27\n",
      "         164       0.00      0.00      0.00        15\n",
      "         165       0.00      0.00      0.00        11\n",
      "         166       0.00      0.00      0.00        11\n",
      "         167       0.00      0.00      0.00        12\n",
      "         168       0.00      0.00      0.00         6\n",
      "         169       0.00      0.00      0.00        11\n",
      "         170       0.00      0.00      0.00         5\n",
      "         171       0.00      0.00      0.00        10\n",
      "         172       0.00      0.00      0.00         5\n",
      "         173       0.00      0.00      0.00        13\n",
      "         174       0.00      0.00      0.00         4\n",
      "         175       0.00      0.00      0.00        10\n",
      "         176       0.00      0.00      0.00         8\n",
      "         177       0.00      0.00      0.00         6\n",
      "         178       0.00      0.00      0.00         6\n",
      "         179       0.00      0.00      0.00         4\n",
      "         180       0.00      0.00      0.00         8\n",
      "         181       0.00      0.00      0.00         4\n",
      "         182       0.00      0.00      0.00         1\n",
      "         184       0.00      0.00      0.00         2\n",
      "         186       0.00      0.00      0.00         4\n",
      "         187       0.00      0.00      0.00         1\n",
      "         189       0.00      0.00      0.00         1\n",
      "         190       0.00      0.00      0.00         1\n",
      "         191       0.00      0.00      0.00         1\n",
      "         193       0.00      0.00      0.00         1\n",
      "         194       0.00      0.00      0.00         1\n",
      "         200       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.03    182600\n",
      "   macro avg       0.01      0.02      0.01    182600\n",
      "weighted avg       0.03      0.03      0.03    182600\n",
      "\n",
      "\n",
      "Sample Sales: {'store': 10, 'item': 27}\n",
      "Predicted Sales: [19]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Corrected URL for the dataset\n",
    "url = \"https://github.com/MaliyaPalanikumar/Sales_Prediction/blob/main/train.csv?raw=true\"\n",
    "sales_data = pd.read_csv(url)\n",
    "\n",
    "# Features and target variable\n",
    "X = sales_data[['store', 'item']]\n",
    "y = sales_data['sales']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)\n",
    "\n",
    "# Sample prediction\n",
    "sample = X_test.iloc[0:1]  # Keep as DataFrame to match model input format\n",
    "prediction = rf_classifier.predict(sample)\n",
    "\n",
    "# Retrieve and display the sample\n",
    "sample_dict = sample.iloc[0].to_dict()\n",
    "print(f\"\\nSample Sales: {sample_dict}\")\n",
    "print(f\"Predicted Sales: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb721d7-f954-45b9-8e38-43cc4e49e1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7594166-196b-49d9-b6af-b1cc2bffdb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
